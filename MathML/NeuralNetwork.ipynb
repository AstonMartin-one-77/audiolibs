{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network introduction\n",
    "\n",
    "### First step: apply Jacobian for vectors of variables and multivariate chain rule\n",
    "\n",
    "If function has a vector of variables as input and each element of this vector is a function of $t$ hence if we want to get a derivative $\\frac{df}{dt}$ => it can be calculated like this:\n",
    "$$\n",
    "f(x_1,x_2,...) = f(\\vec{x})\n",
    "$$\n",
    "$$\n",
    "[x_1(t) = ..., x_2(t) = ...]\n",
    "$$\n",
    "$$\n",
    "\\frac{df}{dt} = \\frac{df}{dx}*\\frac{dx}{dt} = \\vec{J}*\\frac{dx}{dt}\n",
    "$$\n",
    "\n",
    "In neural network often we use chain of functions. For example:\n",
    "$$\n",
    "f(\\vec{x}) = f(x_1,x_2)\n",
    "$$\n",
    "$$\n",
    "\\vec{x}(\\vec{u}) = \n",
    "\\begin{bmatrix}\n",
    "x_1(u_1,u_2) \\\\\n",
    "x_2(u_1,u_2)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "\\vec{u}(t) = \n",
    "\\begin{bmatrix}\n",
    "u_1(t) \\\\\n",
    "u_2(t)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In this example derivative of function $f(\\vec{x})$ is Jacobian vector, derivative of function $\\vec{u}(t)$ is vector of derivatives. More complicated situation with function $\\vec{x}(\\vec{u})$ - there is each line - Jacobian vector with respect to $\\vec{u}$ variables, each vector is derivative of $\\vec{x}$ elements (functions):\n",
    "$$\n",
    "\\frac{df}{dt} = \\frac{\\delta f}{\\delta \\vec{x}}*\\frac{\\delta \\vec{x}}{\\delta \\vec{u}}*\\frac{d\\vec{u}}{dt} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\delta f}{\\delta x_1} & \\frac{\\delta f}{\\delta x_2}\n",
    "\\end{bmatrix} *\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\delta x_1}{\\delta u_1} & \\frac{\\delta x_1}{\\delta u_2} \\\\\n",
    "\\frac{\\delta x_2}{\\delta u_1} & \\frac{\\delta x_2}{\\delta u_2}\n",
    "\\end{bmatrix} *\n",
    "\\begin{bmatrix}\n",
    "\\frac{du_1}{dt} \\\\\n",
    "\\frac{du_2}{dt}\n",
    "\\end{bmatrix} = \\vec{J_f} *\n",
    "\\begin{bmatrix}\n",
    "\\vec{J_{x_1}} \\\\\n",
    "\\vec{J_{x_2}}\n",
    "\\end{bmatrix} *\n",
    "\\begin{bmatrix}\n",
    "\\frac{du_1}{dt} \\\\\n",
    "\\frac{du_2}{dt}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept of Neural Network\n",
    "\n",
    "Simpliest example of Neural Network is function which called sigma (or activation function), one input (activity), one output and 2 numbers - weight and bias.\n",
    "$$\n",
    "a^1 = \\sigma (w^1 a^0 + b)\n",
    "$$\n",
    "\n",
    "<img src=\"img/Simpliest_NN.png\" alt=\"Alternative text\" style=\"display:block;margin-left: auto;margin-right: auto;width: 30%;\" />\n",
    "\n",
    "Formula describes 1 neuron of Neural Network. If we replace input from scalar to vector, for example with 2 input elements:\n",
    "\n",
    "<img src=\"img/2IN_1N.png\" alt=\"2 inputs 1 neuron\" style=\"display:block;margin-left: auto;margin-right: auto;width: 30%;\" />\n",
    "\n",
    "General formula of 1 Layer Neural Network:\n",
    "\n",
    "$$\n",
    "a^1 = \\sigma (\\vec{w^1}*\\vec{a^0} + b)\n",
    "$$\n",
    "\n",
    "<img src=\"img/1Layer_NN.png\" alt=\"1 Layer NN\" style=\"display:block;margin-left: auto;margin-right: auto;width: 30%;\" />\n",
    "\n",
    "This formula still describes 1 neuron, if we want to add more neurons (or more outputs in this context) => output is vector and input is a matrix like weights for each element of output vector respectively, our formula looks like this:\n",
    "$$\n",
    "\\vec{a^1} = \\sigma (W^1 * \\vec{a^0} + \\vec{b^1})\n",
    "$$\n",
    "\n",
    "This Neural Network has only one layer. If we want to add one more layer, our formula looks like this:\n",
    "\n",
    "$$\n",
    "\\vec{a^1} = \\sigma (W^1 * \\vec{a^0} + \\vec{b^1})\n",
    "$$\n",
    "$$\n",
    "\\vec{a^2} = \\sigma (W^2 * \\vec{a^1} + \\vec{b^2})\n",
    "$$\n",
    "\n",
    "<img src=\"img/2Layer_NN.png\" alt=\"2-Layers NN\" style=\"display:block;margin-left: auto;margin-right: auto;width: 20%;\" />\n",
    "\n",
    "This is example of 2-layer Neural Network. General formula for $L$ layers:\n",
    "$$\n",
    "\\vec{a^L} = \\sigma (W^L * \\vec{a^{L-1}} + \\vec{b^L})\n",
    "$$\n",
    "\n",
    "<img src=\"img/L-Layer_NN.png\" alt=\"L Layer NN\" style=\"display:block;margin-left: auto;margin-right: auto;width: 30%;\" />\n",
    "\n",
    "Sigma is a function (sigmoid) which return active output if input value reached some threshold.\n",
    "\n",
    "<img src=\"img/sigma_f.png\" alt=\"Sigmoid function\" style=\"display:block;margin-left: auto;margin-right: auto;width: 30%;\" />\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "Any Neural Network requires correct weights and bias. For calculating it we need to determine cost function which provides us tool for evaluating of NN output result. Sum of squares is a popular cost function:\n",
    "$$\n",
    "C = \\sum{_i(a_i^L - y_i)^2}\n",
    "$$\n",
    "\n",
    "Here we can apply our knowledge about multivariate chain rule (see above) for calculating derivative of cost function with respect to $\\vec{w}$ vector. Doing it we can calculate a gradient for searching a minimum of cost function.\n",
    "\n",
    "<img src=\"img/cost.png\" alt=\"2 inputs 1 neuron\" style=\"display:block;margin-left: auto;margin-right: auto;width: 30%;\" />\n",
    "\n",
    "Let's repeat all functions which we need to consider for calculating derivatives, for example 1-layer NN:\n",
    "$$\n",
    "z^1 = \\vec{w^1}*\\vec{a^0} + b\n",
    "$$\n",
    "$$\n",
    "a^1 = \\sigma (z^1)\n",
    "$$\n",
    "$$\n",
    "C = \\sum{_i(a_i^1 - y_i)^2}\n",
    "$$\n",
    "Great, it looks exactly as chain rule. Derivative of cost function with respect to weights:\n",
    "$$\n",
    "\\frac{\\delta C}{\\delta w} = \\frac{\\delta C}{\\delta a^1}*\\frac{\\delta a^1}{\\delta z^1}*\\frac{\\delta z^1}{\\delta w}\n",
    "$$\n",
    "Derivative of cost function with respect to bias:\n",
    "$$\n",
    "\\frac{\\delta C}{\\delta b} = \\frac{\\delta C}{\\delta a^1}*\\frac{\\delta a^1}{\\delta z^1}*\\frac{\\delta z^1}{\\delta b}\n",
    "$$\n",
    "General form of derivatives:\n",
    "$$\n",
    "\\frac{\\delta C}{\\delta w} = \\frac{\\delta C}{\\delta a^L}*\\frac{\\delta a^L}{\\delta z^L}*\\frac{\\delta z^L}{\\delta w}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\delta C}{\\delta b} = \\frac{\\delta C}{\\delta a^L}*\\frac{\\delta a^L}{\\delta z^L}*\\frac{\\delta z^L}{\\delta b}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Neural Network example\n",
    "\n",
    "N-hidden layers, one input and one output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5180427613584655\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, numHidLayers, layerSz):\n",
    "        self.NN_w = np.ndarray((numHidLayers,layerSz,layerSz), dtype=\"float\")\n",
    "        self.NN_w.fill(0.01)\n",
    "        self.NN_b = np.ndarray((numHidLayers,layerSz), dtype=\"float\")\n",
    "        self.NN_b.fill(0.01)\n",
    "        self.NN_sw = np.ndarray((layerSz), dtype=\"float\")\n",
    "        self.NN_sw.fill(0.01)\n",
    "        self.NN_sb = 0.01\n",
    "        self.NN_lw = np.ndarray((layerSz), dtype=\"float\")\n",
    "        self.NN_lw.fill(0.01)\n",
    "        self.NN_lb = 0.01\n",
    "        self.tmpVector = np.ndarray(layerSz, dtype=\"float\")\n",
    "        self.inputVector = np.ndarray(layerSz, dtype=\"float\")\n",
    "\n",
    "    def calc(self, inputValue):\n",
    "        def sig(x): return 1/(1+np.exp(-x))\n",
    "        for row in range(len(self.NN_sw)):\n",
    "            self.inputVector[row] = sig(inputValue * self.NN_sw[row] + self.NN_sb)\n",
    "        for l in range(len(self.NN_w)):\n",
    "            for row in range(len(self.NN_w[l])):\n",
    "                self.tmpVector[row] = self.inputVector @ self.NN_w[l][row]\n",
    "                self.tmpVector[row] += self.NN_b[l][row]\n",
    "            self.inputVector[:] = sig(self.tmpVector[:])\n",
    "        return sig(self.inputVector @ self.NN_lw + self.NN_lb)\n",
    "\n",
    "test = NeuralNetwork(1, 12)\n",
    "print(test.calc(10.5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our Neural network\n",
    "\n",
    "For doing it we need to apply backpropagation method to weights and bias. For example there is 3-Layer NN:\n",
    "$$\n",
    "\\frac{\\delta C}{\\delta w^3} = \\frac{\\delta C}{\\delta a^3}*\\frac{\\delta a^3}{\\delta z^3}*\\frac{\\delta z^3}{\\delta w^3}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\delta C}{\\delta a^3} = 2*(a^3 - y)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\delta a^3}{\\delta z^3} = (1+e^{-x})^{-1} = e^{-x} * (1+e^{-x})^{-2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\delta z^3}{\\delta w^3} = a^2\n",
    "$$\n",
    "If we want to calculate derivative of cost function with respect to $w^2$, we need to add partial derivative of z^3 with respect to a^2.\n",
    "$$\n",
    "\\frac{\\delta C}{\\delta w^2} = \\frac{\\delta C}{\\delta a^3}*\\frac{\\delta a^3}{\\delta z^3}*\\frac{\\delta z^3}{\\delta a^2}*\\frac{\\delta a^2}{\\delta z^2}*\\frac{\\delta z^2}{\\delta w^2}\n",
    "$$\n",
    "If we want to calculate derivative of cost function with respect to $w^1$, we need to add partial derivative of z^2 with respect to a^1.\n",
    "$$\n",
    "\\frac{\\delta C}{\\delta w^1} = \\frac{\\delta C}{\\delta a^3}*\\frac{\\delta a^3}{\\delta z^3}*\\frac{\\delta z^3}{\\delta a^2}*\\frac{\\delta a^2}{\\delta z^2}*\\frac{\\delta z^2}{\\delta a^2}*\\frac{\\delta a^2}{\\delta z^2}*\\frac{\\delta z^2}{\\delta w^1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_sig(x):\n",
    "    tmpVal = np.exp(-x)\n",
    "    return tmpVal / ((1+tmpVal)**2)\n",
    "\n",
    "def latest_deriv_w(weights, desireA, lA, preA):\n",
    "    lA-= desireA\n",
    "    lA *= 2*d_sig(lZ)*preA\n",
    "    return lA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
